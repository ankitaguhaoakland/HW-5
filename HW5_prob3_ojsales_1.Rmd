---
title: "HW5 - Problem 3 - Orange Juice classification"
author: "misken"
date: "March 26, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem 3 - Predicting orange juice purchases

The dataset is available as part of the ISLR package. You can see the
documentation for that package or the following link describes the OJ
dataset - https://rdrr.io/cran/ISLR/man/OJ.html.

**SUGGESTION**: See the material available in Downloads_StatModels2 from the
session on classification models in R. In particular, the folder on
logistic regression and the example in the folder intro_class_HR/ will
be useful.

## Data prep

We'll do a little data prep to set things up so that we are trying to
predict whether or not the customer purchased Minute Maid (vs Citrus Hill.)
Just run the following chunks to load the dataset, do some data prep and
then partition the data into training and test sets.

```{r loaddata}
ojsales <- (ISLR::OJ)
```

Loading a few libraries
```{r}
library(dplyr)
library(ggplot2)
library(useful)
library(coefplot)
library(caret)
library(lattice)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(class)
library(randomForest)
```

Clean up the storeid related fields. Drop Store7 field.

```{r}
ojsalesdf <- ojsales[ojsales$StoreID != "7",]

# Omitting NA's in the dataframe
na.omit(ojsalesdf)
```


```{r factors}
ojsalesdf$StoreID <- as.factor(ojsalesdf$StoreID)

# Create a new variable to act as the response variable.
ojsalesdf$MM <- as.factor(ifelse(ojsalesdf$Purchase=="MM",1,0))
```

Now we'll just take a subset of the columns as there are a few that contain the
same information. Remember, the new column `MM` is the one we are trying to
predict.

```{r subset}
ojsalesdf_subset <- ojsalesdf[, c(19, 3:13, 15:17)]
```

Just run this chunk to create training and test datasets. This way we'll
all be working with the same datasets. Notice that the test set is 10% of
the full dataset.

```{r partition}
set.seed(167)
sample_size <- ceiling(0.10 * nrow(ojsalesdf))
testrecs <- sample(nrow(ojsalesdf_subset),sample_size)
ojsalesdf_test <- ojsalesdf_subset[testrecs,]
ojsalesdf_train <- ojsalesdf_subset[-testrecs,]  # Negative in front of vector means "not in"
rm(ojsalesdf_subset, ojsalesdf) # No sense keeping a copy of the entire dataset around
```

## Your job

# EDA

Let's check if we really got rid of the StoreID 7 from our dataframe. Yup, we did! 
```{r}
head(ojsalesdf_train)
```

Let's check our dataframe type now
```{r}
str(ojsalesdf_train)
```

Let's take a look at the Summary of our dataframe 
```{r}
summary(ojsalesdf_train)
```

Let's try to fit our Model using Logistic Regression.

# Model 1: Logistic Regression
```{r}
MM_LogM1 <- glm(MM ~  .,
                    data=ojsalesdf_train, family=binomial(link="logit"))

summary(MM_LogM1)
```

Since some of our co-efficients in the Model has NA values and for obvious reasons they does not look like to be  contributing for our Model significance, let's remove them from our Model and fit our Model.

```{r}
MM_LogM1 <- glm(MM ~  . - SalePriceMM - ListPriceDiff - SalePriceCH - PriceDiff,
                    data=ojsalesdf_train, family=binomial(link="logit"))

summary(MM_LogM1)
```

Converting fitted values to predictions. We are predicting at 95% confidence interval, hence we are 
```{r}
yhat_LogM1 <- (MM_LogM1$fit > 0.5) * 1
```

Putting the fitted and the predicted values into a dataframe for ease of analysis
```{r}
MM_fit_predictions <- data.frame(MM_LogM1$y, yhat_LogM1)
names(MM_fit_predictions) <- c("yact","yhat_LogM1")
```

## Creating Confusion Matrix
```{r}
# Confusion Matrix
table(MM_fit_predictions$yact, MM_fit_predictions$yhat_LogM1)

# Percentage of Precision and Recall 
prop.table(table(MM_fit_predictions$yact, MM_fit_predictions$yhat_LogM1))
```

Looks like our Model has an Accuracy of 82% and the Sensitivity or the Probability of True Positive is 80% at the 95% confidence interval.
```{r}
cm_LogM1 <- confusionMatrix(MM_fit_predictions$yhat_LogM1, MM_fit_predictions$yact, 
                            positive = "1")
cm_LogM1
```

Visualizing the rate of purchase of MM vs CH. We got an almost ideal distribution scores with the scores of negative instances (0) to the left represented by the Orange Line and the distribution of positive instances (1) to the right represented by the dotted Green Line. This Orange solid line represents the distribution of the customer not buying MM or in other words the distribution of the customer buying CH and the dotted Green line represents the distributin of the customer buying MM or in other words the distribution of the customers not buying CH. 

And this distribution is actually correct as we can see from the Confusion Matrix as well.
```{r}
ggplot(MM_fit_predictions, aes(x=yhat_LogM1, y=yact)) + geom_point() + 
stat_smooth(method="glm", family="binomial", se=FALSE)

# Double Density Plot
ggplot(ojsalesdf_train, aes(x=yhat_LogM1, color=MM, linetype=MM)) + geom_density()
```

# Model 2: Simple Decision Tree
```{r}
MM_tree <- rpart(MM ~ ., data=ojsalesdf_train, method="class")

# Visualize the tree with rpart.plot
rpart.plot(MM_tree)
```

## Creating Confusion Matrix
```{r}
MM_fit_predictions$yhat_tree <- predict(MM_tree, type = "class")

# Confusion Matrix
table(MM_fit_predictions$yact, MM_fit_predictions$yhat_tree)

# Percentage of Precision and Recall 
prop.table(table(MM_fit_predictions$yact, MM_fit_predictions$yhat_tree))
```

```{r}
cm_tree <- confusionMatrix(MM_fit_predictions$yhat_tree, MM_fit_predictions$yact, 
                            positive = "1")
cm_tree
```

You should build at least two classification models to try to predict MM.
Our error metric will be overall accuracy.

Obviously, `ojsales_train` is your training dataset. After fitting each
model, use the `caret::confusionMatrix` function to create a confusion matrix
for each of the models based on the training data.

You should at least try the following two techniques:
- logistic regression
- a simple decision tree

**HACKER EXTRA:** Try additional techniques such as random forest, k-nearest 
neighbor or others.

# Model 3: Random Forest
```{r}
rand_forest <- randomForest(MM ~ ., data=ojsalesdf_train, method = "class")
```

## Creating Confusion Matrix
```{r}
#MM_fit_predictions$yhat_rf <- predict(rand_forest, type = "class")
# Confusion matrix
table(MM_fit_predictions$yact, MM_fit_predictions$yhat_rf)

# Percentage of Precision and Recall 
prop.table(table(MM_fit_predictions$yact, MM_fit_predictions$yhat_rf))
```

```{r}
cm_rf <- confusionMatrix(MM_fit_predictions$yhat_rf, MM_fit_predictions$yact, 
                            positive = "1")
cm_rf
```







Then use the `predict()` function to make classification predictions on the
test dataset and use `caret::confusionMatrix` to create a confusion matrix
for each of the models for the predictions. 

Summarize your results. 
- Which technique performed the best in terms of overall accuracy? 
- Which technique had the best sensitivity score?
- How did accuracy differ for the training and test datasets for each model?
- Is their any evidence of overfitting?